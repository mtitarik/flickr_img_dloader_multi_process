{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo Source URL description: https://www.flickr.com/services/api/misc.urls.html\n",
    "\n",
    "# Find the id of a group by group name: https://www.flickr.com/services/api/explore/flickr.urls.lookupGroup\n",
    "# Get photos from a group: https://www.flickr.com/services/api/explore/flickr.groups.pools.getPhotos\n",
    "\n",
    "# flickr photo search api: https://www.flickr.com/services/api/explore/flickr.photos.search\n",
    "\n",
    "# Flickr photo licenses explained: https://www.flickr.com/creativecommons/\n",
    "# Flick photo licenses: https://www.flickr.com/services/api/explore/flickr.photos.licenses.getInfo\n",
    "\n",
    "# sample download scripts: https://gist.github.com/zmwangx/b1c16b197b5416143c7a\n",
    "#                          https://www.programcreek.com/python/example/6468/flickrapi.FlickrAPI\n",
    "#% reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from multiprocessing import Pool\n",
    "import flickrapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-71f9eb60a1ba>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-71f9eb60a1ba>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    api_key = #''\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "api_key = #''\n",
    "secret = #''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flickr_search_base_config(pagenum):\n",
    "    \"\"\"\n",
    "    Generate base param configuration for Flickr API query.\n",
    "    \n",
    "    Params:\n",
    "        pagenum (int): Index (starting from 1) of the result page to fetch.  \n",
    "    \n",
    "    Returns: \n",
    "        A Python dictionary containing the configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    config = dict(per_page=100,\n",
    "                  content_type=1,\n",
    "                  license='1,2,3,4,5,6,9,10',\n",
    "                  extras='url_o,url_c',\n",
    "                  sort='relevance',\n",
    "                 )\n",
    "\n",
    "    # add pagenum to config if has valid value  \n",
    "    if not pagenum is None:\n",
    "        config['page'] = pagenum\n",
    "        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_flickr_img(photos, download_dir='/disks/data/datasets/dloaded/', \n",
    "                        im_size = 'c', max_num = None, show_log = False, worker_id = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Downloads images from Flickr.\n",
    "    \n",
    "    Params: \n",
    "        photos (sequence): Sequence of 'photo' elements (python dictionary) from Flickr API image search result.\n",
    "        download_dir (str): Location where the downloaded images will be stored. (current directory).\n",
    "        im_size (str): One of 'm', 'c', 'o', 'b' etc. See https://www.flickr.com/services/api/misc.urls.html for details.\n",
    "        max_num (int): Maximum number of images to download from the given sequence (None). \n",
    "        show_log (Bool): Enables logging (False).\n",
    "        worker_id (int): For tracking the worker this invokation belongs to (None).\n",
    "        \n",
    "    Returns: Does not return any value    \n",
    "    \"\"\"\n",
    "    \n",
    "    if show_log and not worker_id is None:\n",
    "        print(f\"image downloader#{worker_id} received list of {len(photos)} images \")\n",
    "        \n",
    "    # create the download directory if does not exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "        \n",
    "    # limit maximum number of images to be downloaded if was specified\n",
    "    if max_num is None:\n",
    "        pass\n",
    "    else:\n",
    "        photos = photos[:min(max_num, len(photos))]\n",
    "    \n",
    "    # build image URL from component information contained in the photo elements\n",
    "    def build_im_url(photo, im_size):\n",
    "        try:\n",
    "            farm_id = photo['farm']\n",
    "            server_id = photo['server']  \n",
    "            im_id = photo['id'] \n",
    "            secret = photo['secret'] \n",
    "            im_sz = im_size\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        # format: https://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}_[mstzb].jpg\n",
    "        return f'https://farm{farm_id}.staticflickr.com/{server_id}/{im_id}_{secret}_{im_sz}.jpg'  \n",
    "\n",
    "    \n",
    "    for photo in photos:\n",
    "        #img_url = photo.get('url_c') or photo.get('url_o') or build_im_url(photo, im_size)\n",
    "        img_url = build_im_url(photo, im_size)\n",
    "    \n",
    "        # move on if image is not available\n",
    "        if img_url is None:\n",
    "            continue\n",
    "        \n",
    "        # build image name from location directory and image id\n",
    "        img_name = os.path.join(download_dir, img_url.split('/')[-1])\n",
    "        \n",
    "        # download and save image\n",
    "        urlretrieve(img_url, img_name)\n",
    "        \n",
    "        if show_log:\n",
    "            print('Downloaded '+img_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_flickr_img_url_by_tag(tags, pagenum = None, show_log = False):\n",
    "    \"\"\"\n",
    "    Downloads information about images matching the tag(s) \n",
    "    \n",
    "    Params:\n",
    "        tags (str): Comma separated tag words to search with\n",
    "        #tags_conjunct (Bool): Indicates if intended tag combination is 'AND' (True).\n",
    "        #search_title (Bool): Indicates if should search within the image title (True).\n",
    "        pagenum (int): Index (starting from 1) of the result page to fetch (None).                       \n",
    "        show_log (Bool): Enables logging (False).\n",
    "    \n",
    "    Returns: \n",
    "        dictionary containing the query result\n",
    "    \"\"\"\n",
    "    \n",
    "    # search with 'AND'ed tags (conjunction of tags)\n",
    "    tags_conjunct = True\n",
    "    \n",
    "    # do search within title\n",
    "    search_title = True \n",
    "    \n",
    "    \n",
    "    # initialize flickr api\n",
    "    flickr = flickrapi.FlickrAPI(api_key, secret,format='parsed-json',\n",
    "                               store_token=False,\n",
    "                               cache=True)\n",
    "    \n",
    "    # generate base configuration for the flickr query    \n",
    "    config = get_flickr_search_base_config(pagenum)\n",
    "    \n",
    "    # add tag query specific params\n",
    "    if search_title:\n",
    "        config['text'] = \" \".join(tags.split(\",\"))\n",
    "        \n",
    "    config['tags'] = tags\n",
    "    config['tag_mode'] = 'all' if tags_conjunct else 'any' \n",
    "\n",
    "    # fetch image infos\n",
    "    result = flickr.photos.search(**config)\n",
    "    \n",
    "    if(show_log):\n",
    "        print(f\"Downloading page: {pagenum}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def download_flickr_img_by_group(group_name, pagenum = None, show_log = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Downloads information about images from a given Flickr group\n",
    "    \n",
    "    Params: \n",
    "        group_name (str): Name of the flickr group.\n",
    "        pagenum (int): Index (starting from 1) of the result page to fetch (None).                       \n",
    "        show_log (Bool): Enables logging (False).\n",
    "    \n",
    "    Returns: \n",
    "        dictionary containing the query result \n",
    "    \"\"\"\n",
    "\n",
    "    # initialize flickr api\n",
    "    flickr = flickrapi.FlickrAPI(api_key, secret,format='parsed-json', store_token=False, cache=True)\n",
    "    \n",
    "    # look up the id of the group by provided group name\n",
    "    groupinfo = flickr.urls.lookupGroup(url='https://www.flickr.com/groups/' + group_name + '/')\n",
    "    \n",
    "    # extract the group id from the result\n",
    "    group_id = groupinfo.get('group').get('id')\n",
    "        \n",
    "    # generate base configuration for the flickr query    \n",
    "    config = get_flickr_search_base_config(pagenum)\n",
    "\n",
    "    # add the group id to the config\n",
    "    config['group_id'] = group_id\n",
    "    \n",
    "    # retrieve information of the photos from the group\n",
    "    result = flickr.groups.pools.getPhotos(**config)\n",
    "\n",
    "    if(show_log):   \n",
    "        print(f\"Downloading page: {pagenum}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11546858/python-multiprocessing-keyword-arguments\n",
    "# https://stackoverflow.com/questions/34031681/passing-kwargs-with-multiprocessing-pool-map\n",
    "\n",
    "def downloader_wrapper(arg):\n",
    "    \"\"\"\n",
    "    Wrapper for the image downloader.\n",
    "    \n",
    "    Params:\n",
    "        arg (tuple): arguments (iterable, kwargs) to be passed to the image downloader.\n",
    "        \n",
    "    Returns: forwards image downloader result.     \n",
    "    \"\"\"\n",
    "    \n",
    "    args, kwargs = arg\n",
    "    \n",
    "    return download_flickr_img(args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_args(num_processes, query_photos_dict, max_img_dload_cnt_per_worker, download_dir, im_size, max_num):\n",
    "    \"\"\"\n",
    "        Creates a list of arguments for the image downloader processes\n",
    "        \n",
    "        Params:\n",
    "            num_processes (int): The intended number of image downloader processes\n",
    "            query_photos_dict (dictionary): Containing Flickr ids of the images as keys and 'photo' elements as values \n",
    "                               that contain information such as farm-id, secret, etc.\n",
    "            max_img_dload_cnt_per_worker (int): The maximum number of images to be assigned to a worker process.\n",
    "            download_dir (str): The base directory for saving the downloaded images. Each process has its own subdirectory\n",
    "                          under this dir in which it saves the images.\n",
    "            im_size (str): One of the available image sizes (e.g. 'c', 'm', 'b', 'o') in Flickr.\n",
    "            \n",
    "        Returns:\n",
    "            A list, each element of which corresponds to the argument to be passed to an image downloader process. \n",
    "    \"\"\"\n",
    "    \n",
    "    arg = []\n",
    "    \n",
    "    # Retrieve the photo elements as a list\n",
    "    query_photos = list(query_photos_dict.values())\n",
    "    \n",
    "    for i in range(num_processes):\n",
    "        # create the keyword arguments for the processes\n",
    "        kwargs = {   \n",
    "                     'download_dir' : os.path.join(download_dir,str(i+1)),\n",
    "                     'im_size' : im_size,\n",
    "                     'max_num' : max_num,\n",
    "                     'worker_id': i+1,\n",
    "                 }\n",
    "        \n",
    "        # compute the section end points' of the photo element list to be used for this worker process    \n",
    "        photo_start_idx = max_img_dload_cnt_per_worker*i\n",
    "        photo_end_idx = max_img_dload_cnt_per_worker*(i+1)\n",
    "        \n",
    "        # The regular argument and the keyword argument will be sent as argument to a process\n",
    "        arg.append((query_photos[photo_start_idx:photo_end_idx], kwargs))        \n",
    "        \n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages needed for 600 images: 6\n",
      "image_info_save_file does not exist\n",
      "Total available images: 48431\n",
      "Total available pages: 485\n",
      "Downloading page: 1\n",
      "Downloading page: 2\n",
      "Downloading page: 3\n",
      "Downloading page: 4\n",
      "Downloading page: 5\n",
      "Downloading page: 6\n",
      "Obtained total 600 image information\n",
      "Attempting to download total 600 images\n",
      "Number of image downloaders: 3\n",
      "Elapsed time 121.71158790588379 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recommended way for creating a multi-process program in python\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    # number of images to download\n",
    "    num_images_to_download = 500 \n",
    "\n",
    "    # Specify whether to download from a group or by tags\n",
    "    config = 'tags' # possible values: 'group' or 'tags'\n",
    "\n",
    "    # base download directory\n",
    "    download_dir = f'{gdrive_root}/dataset/test_dataset'\n",
    "\n",
    "\n",
    "    # name of the group (if downloading from a group, ignored otherwise)\n",
    "    group_name = 'portrait-gallery'\n",
    "\n",
    "    # tags to be used to searching (if downloading using search tags, ignored otherwise)\n",
    "    tags = 'tigers'\n",
    "\n",
    "    \n",
    "    # maximum number of images to assign to an image downloader process\n",
    "    MAX_LOAD_PER_IMG_DLOADER = 200 \n",
    "    \n",
    "    # maximum number of images information to be contained in a query result page.\n",
    "    # NOTE: Flickr sends the same set of image information if used a number higher than 100.\n",
    "    MAX_IMG_PER_PAGE = 100    \n",
    "    \n",
    "    # hack since api downloads 100 less than the specified number of images\n",
    "    num_images_to_download +=100 \n",
    "    \n",
    "    # finds the number of pages needed to be downloaded\n",
    "    num_pages = int(math.ceil(num_images_to_download/MAX_IMG_PER_PAGE))\n",
    "\n",
    "    print(f'Number of pages needed for {num_images_to_download} images: {num_pages}')\n",
    "    \n",
    "    # settings for downloading images from a group or by search keywords\n",
    "    configs = { 'group': {\n",
    "                            'search_criteria' : group_name,\n",
    "                            'download_dir' : download_dir,    \n",
    "                            'image_info_save_file' : 'query_results_bygroup__' + group_name,\n",
    "                            'url_downloader' : download_flickr_img_by_group, # image info downloader function for groups\n",
    "                        },\n",
    "               'tags': {\n",
    "                            'search_criteria' : tags,\n",
    "                            'download_dir' : download_dir,    \n",
    "                            'image_info_save_file' : 'query_results_bytags__' + \".\".join(tags.split(\",\")),\n",
    "                            'url_downloader' : download_flickr_img_url_by_tag, # image info downloader function for tags\n",
    "                        }\n",
    "              }\n",
    "    \n",
    "    # download image information if no saved version exists on disk\n",
    "    if not os.path.exists(configs[config]['image_info_save_file']):\n",
    "        print('image_info_save_file does not exist')\n",
    "        \n",
    "        # execute the query for finding out the number of pages available \n",
    "        result = configs[config]['url_downloader'](configs[config]['search_criteria'])\n",
    "\n",
    "        # Number of pages in the result    \n",
    "        pagecount = result['photos']['pages']\n",
    "\n",
    "        print(f\"Total available images: {result['photos']['total']}\")\n",
    "        print(f'Total available pages: {pagecount}')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # download image info (can't be done parallely since can't instantiate more than one Flickr api instances with \n",
    "        # the same api key)\n",
    "        for i in range( min(num_pages, pagecount) ):\n",
    "            \n",
    "            # download image info as a dictionary and append to a list  \n",
    "            results.append(configs[config]['url_downloader'](configs[config]['search_criteria'], i+1, show_log = True))\n",
    "\n",
    "            # enforced delay not to overwhelm the Flickr API  \n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # construct a dictionary of returned photos to eliminate ducplicate images (based on image id)      \n",
    "        query_photos = { photo['id']:photo for result in results for photo in result['photos']['photo']}\n",
    "\n",
    "        # compute the number of image information obtained       \n",
    "        actual_total_image_count = len(query_photos.items()) # Flickr returns 100 less images for some reason\n",
    "\n",
    "        print(f'Obtained total {actual_total_image_count} image information')\n",
    "\n",
    "        # save the search result dictionary to disk to save time should we need to re-download      \n",
    "        pickle.dump(query_photos, open(configs[config]['image_info_save_file'],'wb'))\n",
    "\n",
    "    else:                  \n",
    "        print('image_info_save_file exists')\n",
    "              \n",
    "        # load saved search result containing photo elements\n",
    "        query_photos = pickle.load(open(configs[config]['image_info_save_file'],'rb'))      \n",
    "    \n",
    "        print(f\"Loaded image info file from disk for {config} {configs[config]['search_criteria']}\")\n",
    "              \n",
    "        # compute the number of image information obtained       \n",
    "        actual_total_image_count = len(query_photos.items()) # Flickr returns 100 less images for some reason\n",
    "        \n",
    "        if( num_images_to_download < actual_total_image_count):\n",
    "            im_ids = list(query_photos.keys())\n",
    "            im_ids = im_ids[:num_images_to_download]\n",
    "            photos = list(query_photos.values())\n",
    "            photos = photos[:num_images_to_download]\n",
    "              \n",
    "            query_photos = dict(zip(im_ids, photos))\n",
    "              \n",
    "            actual_total_image_count = num_images_to_download  \n",
    "    \n",
    "    print(f'Attempting to download total {actual_total_image_count} images')          \n",
    "              \n",
    "    # update num_process to reflect actual image count\n",
    "    num_processes = int(math.ceil(actual_total_image_count/MAX_LOAD_PER_IMG_DLOADER))\n",
    "              \n",
    "    print(f'Number of image downloaders: {num_processes}')\n",
    "              \n",
    "    # arrange the image downloader arguments for multi-process environment          \n",
    "    args = get_worker_args(num_processes, query_photos, MAX_LOAD_PER_IMG_DLOADER, configs[config]['download_dir'], \n",
    "                            'c', None)\n",
    "    \n",
    "    # for suppressing process output\n",
    "    def mute():\n",
    "        sys.stdout = open(os.devnull, 'w')   \n",
    "              \n",
    "    start = time.time()\n",
    "              \n",
    "    with Pool(num_processes) as p:\n",
    "        p.map(downloader_wrapper, args);\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Elapsed time {end-start} seconds')\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
